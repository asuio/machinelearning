{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 感知机\n",
    "感知机是二分类的线性分类模型，输入空间是特征向量，输出为示例的类别，取+1和-1。\n",
    "在感知机中，将特种空间中的实例划分为某个超平面的两侧，感知机的问题就是求解这个超平面。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设特征空间是 $\\chi \\subseteq R^{n} $，输出空间 $\\varphi$={+1，-1},输入$x \\in \\chi$，表示实例的特征向量，输出 $y \\in\\varphi$,表示输出的类别。通过这些，我们可以得到下面的这个函数。\n",
    "$$f(x)=sign(\\omega \\cdot x + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个公式中，$\\omega$和$b$称为感知机的模型参数，$\\omega \\in R^n$叫做权值向量，$b \\in R$叫做偏执，$\\omega\\cdot x$表示$\\omega$和$x$的内积。sign是符号函数：\n",
    "$$ sign(x) = \\left\\{\n",
    "\\begin{aligned}\n",
    "+1 && x\\geq 0 \\\\\n",
    "-1 && x < 0\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/linear_seq.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图，这个图形中有两种类别的数据，我们需要做的就是找到这样的一个面能够很好的将两种数据分割开，这就是感知机的作用与目的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "根据上面的$f(x)$，我们发现，当$sign(\\omega \\cdot x + b)$的返回值是-1时，$sign(\\omega \\cdot x + b) < 0$ ,反之，$sign(\\omega \\cdot x + b)>0$。这样我们就定义下面的这个方程:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$\\omega \\cdot x + b=0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方程对应特征空间$R^n$上的一个超平面，这个超平面的两侧，分别是正负两类样本。其中 $\\omega$是这个超平面的法向量，b是这个超平面的截距。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 补充：$\\omega$为什么是平面$\\omega \\cdot x + b = 0$的法向量\n",
    "在这个平面中找任意的点 $x_1$和$x_2$,我们可以得到这样两个等式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\omega \\cdot x_1 + b = 0 \\quad  (1) $$\n",
    "$$ \\omega \\cdot x_2 + b = 0 \\quad  (2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方程(1)-方程(2)=$\\omega \\cdot (x_1 -x_2) = 0$，这里我们可以看到，平面上任意的$x_1$和$x_2$与$\\omega$的乘积都为0，所以$\\omega$就是这个平面的法向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设数据集$T =\\{(x_1,y_1),(x_2,y_2),,...,(x_n,y_n)\\}$， $x_i \\in\\chi \\subseteq R^{n} $，$y_i \\in\\varphi = \\{+1,-1\\}, i=1,2,3...n$，线性可分，即存在某个超平面，能够将这两类完全分开。超平面为：\n",
    "$$\\omega \\cdot x + b=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们现在所要做的事情就是，找出这个超平面，也就是求出$\\omega$和b这两个参数。这样就有$y_i = -1$时，$\\omega x_i +b < 0$，反之，$\\omega x_i +b > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在求这个超平面的过程中，我们可以先假定一个平面，$\\omega_c \\cdot x + b_c = 0$，我们先来讨论以这个超平面分数据分错的情况，在分错的情况下，当 $y_i > 0$时，$\\omega_c \\cdot x_i + b_c < 0$； $y_i < 0$时，$\\omega_c \\cdot x_i + b_c > 0$。这样我们就得到了一个更加简单的公式，去判断这个平面是否是超平面："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$$y_i(\\omega_c \\cdot x_i + b_c) < 0 \\quad (3)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当(3)成立的时候，就说明当前的这个超平面，不能完全的将数据集正确的分开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样我们就知道了怎么判断这个平面是不是我们所需要的超平面了。下面将要介绍如果这个平面不是我们所需要的超平面，我们应该怎么调整这个超平面，即怎么去更新$\\omega$和b。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 点到超平面的距离\n",
    "\n",
    "设$x_0$是空间中的一点，$S$是超平面，点到平面的距离为：\n",
    "$$\\frac{1}{\\parallel \\omega \\parallel_2} |\\omega \\cdot x_i + b|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- 损失函数\n",
    "\n",
    "为了求得超平面的参数$\\omega$和b，需要去定义一个损失函数，利用损失函数去计算参数。在我们设计的损失函数的时候，首先想到的就是误分类的总点数，但是这样的损失函数与$\\omega$和b是无关的，也就不是$\\omega$和$b$的连续可导函数，我们没有办法使用这个损失函数去求解$\\omega$和b。这样我们选择误分类的点到当前超平面的总距离来当做损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据上面的公式(3)我们能保证在分类错误的情况下，下面的公式成立：$$-y_i(\\omega \\cdot x_i + b) > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，误分类的点到直线的距离就是：\n",
    "$$-\\frac{1}{\\parallel \\omega \\parallel_2}y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，我们假设超平面$S$误分类的点的集合为$M$，那么所有的误分类的点到平面$S$的距离的总和就是：\n",
    "$$-\\frac{1}{\\parallel \\omega \\parallel_2}\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不考虑$\\frac{1}{\\parallel \\omega \\parallel_2}$，我们就得到了感知机的损失函数：\n",
    "$$L(\\omega, b) = -\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个就是感知机学习的经验风险函数。在这个损失函数中我们可以看到，$L(\\omega, b)$一定是非负的，在有误分类的情况下，就是参数$\\omega$和$b$的线性函数，即损失函数$L(\\omega, b)$是$\\omega$和$b$的连续可导函数。误分类的点越少，$L(\\omega, b)$的值就越小，当没有误分类的点的时候，损失函数的值为0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上面的分析可知，当损失函数的值越小，我们得到的超平面的分类效果就越好，所以我们就可以把超平面的计算，改为，求解 $\\omega$和$b$，使得损失函数最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min\\limits_{\\omega,b}L(\\omega, b) =-\\sum_{x_i \\in M}y_i(\\omega \\cdot x_i + b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$M$为误分类的点的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度下降求解$\\omega$和$b$\n",
    "\n",
    "      我会再写一篇文章来介绍梯度下降，这里就不介绍梯度下降的原理了。这里需要我们知道的是，函数沿着梯度的方向前进，函数值增长的最快；沿着梯度的反方向前进，函数值下降的最快。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "首先，我们假设误分类点集合$M$是固定的，那么损失函数$L(\\omega, b)$的梯度为：\n",
    "$$\\triangledown_\\omega L(\\omega, b) = -\\sum_{x_i\\in M}y_ix_i$$\n",
    "$$\\triangledown_b L(\\omega, b) = -\\sum_{x_i\\in M}y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们随机的选择一个误分类点，对$\\omega$和$b$进行更新：\n",
    "$$\\omega \\gets \\omega + \\eta y_i x_i$$\n",
    "$$b \\gets b + \\eta y_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中$\\eta (0 <\\eta \\le 1)$是学习率。这样我们就可以通过迭代使得损失函数$L(\\omega ,b)$不断的变小，直至到0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 感知机的算法过程\n",
    "\n",
    "1.选定初值：$\\omega_0$,$b_0$\n",
    "\n",
    "2.在训练数据集中选择数据$x_i,y_i$\n",
    "\n",
    "3.如果$y_i(\\omega_c \\cdot x_i + b_c) < 0$\n",
    "$$\\omega \\gets \\omega + \\eta y_i x_i$$\n",
    "$$b \\gets b + \\eta y_i$$\n",
    "4.返回（2）,直到训练集中没有误分类的点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 算法收敛性\n",
    "\n",
    "现在我们把偏置$b$加入到权重向量${\\omega}$中，令$\\hat{\\omega} = (\\omega^T, b)^T$，同样也将输入向量扩充，加入常数1，记作$\\hat{x}=(x^T,1)^T$，这样$\\hat{x}\\in R^{n+1}$，$\\hat{\\omega}\\in R^{n+1}$。那么$\\hat{\\omega}\\cdot \\hat{x} = \\omega \\cdot x+ b$。\n",
    "现在我们设训练集$T=\\{(x_1,y_1),(x_2,y_2),\\cdots (x_N, y_N)\\}$线性可分的，其中$x_i \\in R^n$，$y\\in \\{-1, +1\\}$, $i = 1,2,\\cdots,N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当数据集是线性可分的时候，我们可以找到这样的超平面:在$\\frac{1} {\\parallel\\omega \\parallel}_2 = 1$，$\\hat{\\omega}_{opt} \\cdot \\hat{x}=\\omega_{opt} \\cdot x + b_{opt}=0$，这个超平面能够将整个数据集完全分开。并且一定存在一个$\\Upsilon > 0$，对于所有的$i=1,2,\\cdots,N$\n",
    "$$y_i  (\\hat{\\omega}_{opt} \\cdot \\hat{x}_{opt}) = y_i(\\omega_{opt} \\cdot x + b_{opt}) \\geqslant \\Upsilon \\quad (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在令$R=\\max\\limits_{l \\leqslant i \\leqslant N}\\parallel \\hat{x}_i\\parallel$。\n",
    "现在设最大的误分类次数是$K$，$K$将满足下面的条件：\n",
    "$$K \\leqslant (\\frac{R}{\\Upsilon})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面给出具体的证明："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照上面所说的，在数据集线性可分的情况下，可以找到超平面$\\hat{\\omega}_{opt} \\cdot \\hat{x}=\\omega_{opt} \\cdot x + b_{opt}=0$，使$\\frac{1} {\\parallel\\omega \\parallel}_2 = 1$，对于所有的$i=1,2,\\cdots,N$都有\n",
    "$$y_i  (\\hat{\\omega}_{opt} \\cdot \\hat{x}_{opt}) = y_i(\\omega_{opt} \\cdot x_i + b_{opt}) > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以也能找到：\n",
    "$$\\Upsilon=\\min \\limits_{i}\\{y_i(\\omega_{opt} \\cdot x_i + b_{opt})\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使上面的公式(4)成立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果感知算法从$\\hat{\\omega}=0$开始，如果实例被误分类，则更新权重，令$\\hat{\\omega_{k-1}}$是第$K$次误分类之前的扩充向量$\\hat{\\omega_{k-1}}=(\\omega^{T}_{{k-1}},b_{k-1})$（这里在part_4刚开始的地方）。这样第$K$个误分类的条件是\n",
    "$$y_i(\\hat{\\omega_{k-1}}\\cdot \\hat{x_i}) =y_i({\\omega_{k-1}}\\cdot x_i + b_{k-1})\\leqslant 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们对$\\omega$和$b$进行更新：\n",
    "$$\\omega_k \\gets \\omega_{k-1}+\\eta y_i x_i $$\n",
    "$$b_k \\gets b_{k-1}+ \\eta y_i$$\n",
    "这样我们就得到了：\n",
    "$$\\hat{\\omega_k} = \\hat{\\omega_{k-1}}+\\eta y_i \\hat{x_i}\\quad (5)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在到公式(5)的两边都乘以$\\hat {\\omega_{opt}}$\n",
    "$$\\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}}= \\hat{\\omega_{k-1}}\\cdot \\hat {\\omega_{opt}}+\\eta \\cdot \\hat {\\omega_{opt}} y_i \\hat{x_i} \\quad(6)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时，我们可以看到公式(6)等号右边的部分的这部分$\\hat {\\omega_{opt}} y_i \\hat{x_i}$和公式(4)的前半部分其实是一样的，这样我们就能得出下面的公式：$$\\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}}= \\hat{\\omega_{k-1}}\\cdot \\hat {\\omega_{opt}}+\\eta \\cdot \\hat {\\omega_{opt}} y_i \\hat{x_i} \\geqslant \\hat{\\omega_{k-1}}\\cdot \\hat {\\omega_{opt}} + \\eta\\Upsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后由这个依次类推可以得到：\n",
    "$$\\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}} \\geqslant \\hat{\\omega_{k-1}}\\cdot \\hat {\\omega_{opt}} + \\eta\\Upsilon \\geqslant \\hat{\\omega_{k-2}}\\cdot \\hat {\\omega_{opt}} +2 \\eta\\Upsilon \\geqslant \\cdots \\geqslant K\\eta\\Upsilon$$\n",
    "$$\\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}}  \\geqslant K\\eta\\Upsilon \\quad (7)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对公式(5)进行一些操作：\n",
    "$$\\parallel\\hat{\\omega_{k}}\\parallel^2 = \\parallel\\hat{\\omega_{k-1}}\\parallel^2 + 2\\eta y_i \\hat{\\omega_{k-1}}\\cdot x_i + \\eta^2\\parallel\\hat{x_i}\\parallel^2$$\n",
    "因为是在第k次误分类时，所以$2\\eta y_i \\hat{\\omega_{k-1}}\\cdot x_i \\leqslant 0$，所以下面的公式成立：\n",
    "$$\\parallel\\hat{\\omega_{k}}\\parallel^2 \\leqslant \\parallel\\hat{\\omega_{k-1}}\\parallel^2 + \\eta^2\\parallel\\hat{x_i}\\parallel^2$$\n",
    "又因为在上面我们令 $R=\\max\\limits_{l \\leqslant i \\leqslant N}\\parallel \\hat{x}_i\\parallel$，所以下面的公式成立：\n",
    "$$\\parallel\\hat{\\omega_{k}}\\parallel^2 \\leqslant \\parallel\\hat{\\omega_{k-1}}\\parallel^2 + \\eta^2R^2$$\n",
    "依次类推：\n",
    "$$\\parallel\\hat{\\omega_{k}}\\parallel^2 \\leqslant \\parallel\\hat{\\omega_{k-2}}\\parallel^2 + 2\\eta^2R^2$$\n",
    "$$\\cdots$$\n",
    "$$\\parallel\\hat{\\omega_{k}}\\parallel^2 \\leqslant  K\\eta^2R^2 \\quad (8)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合公(7)和公式(8),我们得到：\n",
    "$$K\\eta\\Upsilon \\leqslant \\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}} \\leqslant \\parallel \\hat{\\omega_k} \\parallel \\leqslant \\sqrt{K}\\eta R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写到最后才发现，当时看书的时候这个公式 $\\hat{\\omega_k}\\cdot \\hat {\\omega_{opt}} \\leqslant \\parallel \\hat{\\omega_k} \\parallel $，我竟然没有推一下为什么成立，尴尬...（先写上，等我推出来后再补充上）\n",
    "于是就有：\n",
    "$$K^2 \\Upsilon^2 \\leqslant K R^2$$\n",
    "这就推出了：\n",
    "$$K \\leqslant (\\frac{R}{\\Upsilon})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上这些就是感知机buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：《统计学习方法》、林轩田机器学习课程。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
