## 机器学习1000题

- [1-10](https://zhuanlan.zhihu.com/p/30301789)
    - 1 简要介绍支持向量机
    - 2 简要介绍tensorFlow计算图
    - 3 在k-means或kNN中，我们常用欧式距离来计算最近邻居的
    - 4 2015百度校招面试
    - 5 LR(逻辑斯蒂回归)
    - 6 overfitting怎样解决（过拟合）
    - 7 LR和SVM的练习与区别
    - 8 说说你知道的核函数
    - 9 LR与线性回归的区别与联系
    - 10 （决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别
- [11-20](https://zhuanlan.zhihu.com/p/30378123)
    - 11 XGBoost为什么要用泰勒展开，优势在哪里
    - 12 xgboost如何寻找最优特征？是又放回还是无放回？
    - 13 判别式模型和生成式模型
    - 14 L1和L2的区别
    - 15 L1和L2正则先验分别服从什么分布
    - 16 CNN最成功的应用是在CV，那为什么NLP和Speech的很多问题也可以用CNN解出来？为什么AlphaGo里也用了CNN？这几个不相关的问题的相似性在哪里？CNN通过什么手段抓住了这个共性
    - 17 说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2...，请写出最终的决策公式
    - 18 LSTM结构推导，为什么比RNN好？
    - 19 Google是怎么利用贝叶斯方法，实现"拼写检查"的功能
    - 20 .为什么朴素贝叶斯如此“朴素”？
- [21-30](https://zhuanlan.zhihu.com/p/30781802)
    - 21 大致比较一下plas和LDA的区别
    - 22 简要说说EM算法
    - 23 KNN中的K如何选取
    - 24 防止过拟合的方法（overfitting）
    - 25 机器学习中，为何要经常对数据做归一化
    - 26 深度学习中的归一化问题
    - 27 哪些机器学习算法不需要做归一化处理
    - 28 树形结构为什么不需要归一化
    - 29 数据归一化（或者标准化，注意归一化和标准化不同）的原因
    - 30 请简要说说一个完整机器学习项目的流程
- [31-35](https://zhuanlan.zhihu.com/p/30822203)
    - 31 逻辑斯特回归为什么要对特征进行离散化
    - 32 new 和 malloc的区别
    - 33 hash 冲突及解决办法
    - 34 CRF模型对于HMM和MEMM模型的优势
    - 35 什么是熵
- [36-40](https://zhuanlan.zhihu.com/p/30851628)
    - 36 熵、联合熵、条件熵、相对熵、互信息的定义
    - 37 什么是最大熵
    - 38 有监督学习和无监督学习的区别
    - 39 正则化
    - 40 协方差和相关性有什么区别
- [41-45]()
    - 41 线性分类器与非线性分类器的区别以及优劣
    - 42 数据的逻辑存储结构（如数组，队列，树等）对于软件开发具有十分重要的影响，试对你所了解的各种存储结构从运行速度、存储效率和适用场合等方面进行简要地分析
    - 43 什么是分布式数据库
    - 44 贝叶斯定理
    - 45 #include和#include“filename.h”有什么区别
- [46-50](https://zhuanlan.zhihu.com/p/31067771)
    - 5道选择题
- [51-55](https://zhuanlan.zhihu.com/p/31097559)
    - 51 选择题
    - 52 选择题
    - 53 请用python编写函数find_string，从文本中搜索并打印内容，要求支持通配符星号和问号
    - 54 红黑树的性质
    - 55 sigmoid激活函数
- [56-60](https://zhuanlan.zhihu.com/p/31137810)
    - 56 什么是卷积
    - 57 CNN的池化pool化层
    - 58 生成对抗网络
    - 59 学梵高作画的原理
    - 60 现在有 a 到 z 26 个元素， 编写程序打印 a 到 z 中任取 3 个元素的组合（比如 打印 a b c ，d y z等）
- [61-65](https://zhuanlan.zhihu.com/p/31229539)
    - 61 梯度下降
    - 62 梯度下降法找到的一定是下降最快的方向么
    - 63 牛顿法和梯度下降法的不同
    - 64 拟牛顿法
    - 65 随机梯度下降法的问题和挑战
- [66-70](https://zhuanlan.zhihu.com/p/31258681)
    - 66 共轭梯度法
    - 67 对所有优化问题来说, 有没有可能找到比現在已知算法更好的算法
    - 68 最小二乘法
    - 69 看你T恤上印着：人生苦短，我用Python，你可否说说Python到底是什么样的语言？你可以比较其他技术或者语言来回答你的问题
    - 70 Python是如何进行内存管理的
- [71-75](https://zhuanlan.zhihu.com/p/31305871)
    - 71 请写出一段Python代码实现删除一个list里面的重复元素
    - 72 编程用sort进行排序，然后从最后一个元素开始判断 a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3]
    - 73 Python生成随机数
    - 74 常见的损失函数
    - 75 logistics回归
- [76-80](https://zhuanlan.zhihu.com/p/31337162)
    - 76 cv最近5年的发展
    - 77 深度学习在视觉领域有何前沿进展
    - 78 HashMap与HashTable区别
    - 79 正样本为10w条数据，负样本只有1w条数据
    - 80 矩阵计算复杂度
- [81-85](https://zhuanlan.zhihu.com/p/31366886)
    - 关于朴素贝叶斯的选择题
- [86-90](https://zhuanlan.zhihu.com/p/31444054)
    - 86  已知一组数据的协方差矩阵P,下面关于主分量说法错误的是
    - 87 kmeans的复杂度
    - 88 关于logit 回归和SVM 不正确的是
    - 89 卷积的一个小问题
    - 90 影响聚类算法结果的主要因素
- [91-95](https://zhuanlan.zhihu.com/p/31477592)
    - 91 马氏距离教欧式距离的有点
    - 92 影响K-均值算法的主要因素
    - 93 在统计模式分类问题中，当先验概率未知时，可以使用
    - 94 如果以特征向量的相关系数作为模式相似性测度，则影响聚类算法结果的主要因素有。
    - 95 欧氏距离具有，马氏距离具有
- [96-100](https://zhuanlan.zhihu.com/p/31537794)
    - 96 deeplearning的调参经验（炼丹的经验）
    - 97 RNN的原理
    - 98 什么是RNN
    - 99 RNN是怎么从单层网络一步一步构造的
    - 100 RNN中只能采用tanh而不是ReLu作为激活函数吗